# -*- coding: utf-8 -*-
"""SAM Karachi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PAtTGGKNQ-exMYIr2GM9SAFzSjzdQW4
"""

# Install the required libraries
#SAM
!pip install git+https://github.com/facebookresearch/segment-anything.git
#Transformers
!pip install -q git+https://github.com/huggingface/transformers.git
#Datasets to prepare data and monai if you want to use special loss functions
!pip install datasets
!pip install -q monai
#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)
!pip install patchify

import numpy as np
import matplotlib.pyplot as plt
import tifffile
import os
from patchify import patchify  #Only to handle large images
import random
from scipy import ndimage

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
from skimage.util import view_as_windows
import matplotlib.pyplot as plt

# Assuming your files are in a directory named "data"
data_directory = 'path/to/your/data'
patch_size = 256
step = 256
all_img_patches = []
all_mask_patches = []

# Get a list of image files in the directory
image_files = [f for f in os.listdir(data_directory) if f.endswith('.jpg') and 'mask' not in f]

num = 0
for image_file in image_files:
    # Construct the corresponding mask file name
    mask_file = image_file.replace('slum', 'mask')

    # Read the image
    large_image = cv2.imread(os.path.join(data_directory, image_file))

    # Check if the image is read successfully
    if large_image is not None:
        # Resize the image to (256, 256)
        large_image_resized = cv2.resize(large_image, (600, 600))

        # Read the corresponding mask
        large_mask = cv2.imread(os.path.join(data_directory, mask_file), cv2.IMREAD_GRAYSCALE)

        # Check if the mask is read successfully
        if large_mask is not None:
            # Resize the mask to (256, 256)
            large_mask_resized = cv2.resize(large_mask, (600, 600))

            # Patchify the resized image and mask
            patches_img = view_as_windows(large_image_resized, (patch_size, patch_size, 3), step=step)
            patches_mask = view_as_windows(large_mask_resized, (patch_size, patch_size), step=step)

            all_img_patches.extend(patches_img.reshape(-1, patch_size, patch_size, 3))
            all_mask_patches.extend((patches_mask / 255.).astype(np.uint8).reshape(-1, patch_size, patch_size))

    num += 1
    print (num)


images = np.array(all_img_patches)
masks = np.array(all_mask_patches)

# Create a list to store the indices of non-empty masks
valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]
# Filter the image and mask arrays to keep only the non-empty pairs
filtered_images = images[valid_indices]
filtered_masks = masks[valid_indices]
print("Image shape:", filtered_images.shape)  # e.g., (num_frames, height, width, num_channels)
print("Mask shape:", filtered_masks.shape)

from datasets import Dataset
from PIL import Image

# Convert the NumPy arrays to Pillow images and store them in a dictionary
dataset_dict = {
    "image": [Image.fromarray(img) for img in filtered_images],
    "label": [Image.fromarray(mask) for mask in filtered_masks],
}

# Create the dataset using the datasets.Dataset class
dataset = Dataset.from_dict(dataset_dict)

example = dataset[0]
image = example["image"]
image

import matplotlib.pyplot as plt
import numpy as np

def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

fig, axes = plt.subplots()

axes.imshow(np.array(image))
ground_truth_seg = np.array(example["label"])
show_mask(ground_truth_seg, axes)
axes.title.set_text(f"Ground truth mask")
axes.axis("off")

def get_bounding_box(ground_truth_map):
  # get bounding box from mask
  y_indices, x_indices = np.where(ground_truth_map > 0)
  x_min, x_max = np.min(x_indices), np.max(x_indices)
  y_min, y_max = np.min(y_indices), np.max(y_indices)
  # add perturbation to bounding box coordinates
  H, W = ground_truth_map.shape
  x_min = max(0, x_min - np.random.randint(0, 20))
  x_max = min(W, x_max + np.random.randint(0, 20))
  y_min = max(0, y_min - np.random.randint(0, 20))
  y_max = min(H, y_max + np.random.randint(0, 20))
  bbox = [x_min, y_min, x_max, y_max]

  return bbox

from torch.utils.data import Dataset

class SAMDataset(Dataset):
  def __init__(self, dataset, processor):
    self.dataset = dataset
    self.processor = processor

  def __len__(self):
    return len(self.dataset)

  def __getitem__(self, idx):
    item = self.dataset[idx]
    image = item["image"]
    ground_truth_mask = np.array(item["label"])

    # get bounding box prompt
    prompt = get_bounding_box(ground_truth_mask)

    # prepare image and prompt for the model
    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors="pt")

    # remove batch dimension which the processor adds by default
    inputs = {k:v.squeeze(0) for k,v in inputs.items()}

    # add ground truth segmentation
    inputs["ground_truth_mask"] = ground_truth_mask

    return inputs

from transformers import SamProcessor

processor = SamProcessor.from_pretrained("facebook/sam-vit-base")

from torch.utils.data import DataLoader, random_split

# Define the batch size
batch_size = 4  # You can adjust this to your desired batch size

# Split the dataset into training and validation sets
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create dataloaders
train_dataloader = DataLoader(SAMDataset(dataset=train_dataset, processor=processor), batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(SAMDataset(dataset=val_dataset, processor=processor), batch_size=batch_size, shuffle=False)

batch = next(iter(train_dataloader))
for k,v in batch.items():
  print(k,v.shape)

batch["ground_truth_mask"].shape

from transformers import SamModel

model = SamModel.from_pretrained("facebook/sam-vit-base")

# make sure we only compute gradients for mask decoder
for name, param in model.named_parameters():
  if name.startswith("vision_encoder") or name.startswith("prompt_encoder"):
    param.requires_grad_(False)

from torch.optim import Adam
import monai

# Note: Hyperparameter tuning could improve performance here
optimizer = Adam(model.mask_decoder.parameters(), lr=1e-4, weight_decay=0)

seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')

from tqdm import tqdm
from statistics import mean
import torch
from torch.nn.functional import threshold, normalize

num_epochs = 20

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

model.train()
for epoch in range(num_epochs):
    epoch_losses = []
    for batch in tqdm(train_dataloader):
      # forward pass
      outputs = model(pixel_values=batch["pixel_values"].to(device),
                      input_boxes=batch["input_boxes"].to(device),
                      multimask_output=False)

      # compute loss
      predicted_masks = outputs.pred_masks.squeeze(1)
      ground_truth_masks = batch["ground_truth_mask"].float().to(device)
      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))

      # backward pass (compute gradients of parameters w.r.t. loss)
      optimizer.zero_grad()
      loss.backward()

      # optimize
      optimizer.step()
      epoch_losses.append(loss.item())

    print(f'EPOCH: {epoch}')
    print(f'Mean loss: {mean(epoch_losses)}')

from transformers import SamModel
import torch

# Your model definition and training code

# Save model state dictionary
torch.save(model.state_dict(), 'optimized_model.pth')

from sklearn.metrics import jaccard_score, f1_score

model.eval()
with torch.no_grad():
    iou_scores = []
    dice_scores = []

    for batch in tqdm(val_dataloader):  # Assuming you have a separate validation dataloader
        # Forward pass
        outputs = model(pixel_values=batch["pixel_values"].to(device),
                        input_boxes=batch["input_boxes"].to(device),
                        multimask_output=False)

        # Compute predicted masks
        predicted_masks = torch.sigmoid(outputs.pred_masks.squeeze(1)).cpu().numpy()
        predicted_masks = (predicted_masks > 0.5).astype(np.uint8)

        # Ground truth masks
        ground_truth_masks = batch["ground_truth_mask"].numpy()

        # Flatten masks for metric computation
        predicted_masks_flat = predicted_masks.flatten()
        ground_truth_masks_flat = ground_truth_masks.flatten()

        # Compute IoU
        iou = jaccard_score(ground_truth_masks_flat, predicted_masks_flat, average='micro')
        iou_scores.append(iou)

        # Compute Dice coefficient
        dice = f1_score(ground_truth_masks_flat, predicted_masks_flat, average='micro')
        dice_scores.append(dice)

    # Compute mean scores
    mean_iou = mean(iou_scores)
    mean_dice = mean(dice_scores)

    print(f'Mean IoU: {mean_iou}')
    print(f'Mean Dice Coefficient: {mean_dice}')

from sklearn.metrics import accuracy_score

pixel_accuracy = accuracy_score(ground_truth_masks_flat, predicted_masks_flat)
print(f'Pixel Accuracy: {pixel_accuracy}')

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(ground_truth_masks_flat, predicted_masks_flat, average=None)
recall = recall_score(ground_truth_masks_flat, predicted_masks_flat, average=None)
f1_per_class = f1_score(ground_truth_masks_flat, predicted_masks_flat, average=None)

print(f'Precision per class: {precision}')
print(f'Recall per class: {recall}')
print(f'F1 Score per class: {f1_per_class}')

import numpy as np
from PIL import Image

# Generate 20 random indices from 1 to 950
random_indices = random.sample(range(1, int(len(all_img_patches)*0.19)), 10)

# Iterate over random indices
for i, idx in enumerate(random_indices):
    # Load image
    image = val_dataset[idx]["image"]

    # Get image dimensions
    image_height, image_width = image.size

    # Create a bounding box prompt covering the entire image
    bounding_box_prompt = [[0, 0, float(image_width), float(image_height)]]

    # Prepare image + bounding box prompt for the model
    inputs = processor(image, input_boxes=bounding_box_prompt, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    model.eval()

    # Forward pass
    with torch.no_grad():
        outputs = model(**inputs, multimask_output=False)

    # Apply sigmoid
    single_patch_prediction = torch.sigmoid(outputs.pred_masks.squeeze(1)).cpu().numpy().squeeze()
    single_patch_prediction = (single_patch_prediction > 0.5).astype(np.uint8)

    # Visualization code without overlay (without probability map)
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Plot the first image on the left
    axes[0].imshow(np.array(image), cmap='gray')
    axes[0].set_title("Image")

    # Plot the second image on the right (Prediction)
    axes[1].imshow(single_patch_prediction, cmap='viridis')
    axes[1].set_title("Prediction")

    # Plot the third image on the right (Ground truth mask)
    ground_truth_mask = val_dataset[idx]["label"]
    axes[2].imshow(ground_truth_mask, cmap='viridis')
    axes[2].set_title("Ground Truth Mask")

    # Hide axis ticks and labels
    for ax in axes:
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_xticklabels([])
        ax.set_yticklabels([])

    # Display the images side by side
    plt.show()